These are my notes for Google's Machine Learning Crash Course.

==FRAMING==
Labels typically expressed by variable y
Features represented by x1, x2, ...xn

Labeled example is this: {features, label} (x,y)
Unlabeled example: {features, ?} (x, ?)
Model maps examples to predicted labels: y'

Train on labeled => use on unlabeled (inference)

Regression: predict numeric value

Classification: likelihood of something belonging to category (is this email spam? Is this photo a cat?)


==DESCENDING INTO ML==
Here's how to explore a data relationship and start to predict it:

1. Plot the data
2. Approximate the relationship, such as drawing a line. (This means it's a linear relationship.) This is summed up as a formula:
	y' = b + w_1 + x_1
	y = predicted label
	b = bias (y-intercept)
	w_1 = weight of feature 1 (IE, a slope)
	x_1 = feature (known output)
Insert a new x_1 => infer a new y' temp

TRAINING: the model learns good values for weights and bias from learned examples. Basically, the ML algo wants to build a model that minimizes loss (a penalty). This is called empirical risk minimization.

Loss = 0 if the prediction was dead on, otherwise it's less. The model wants to find a set of weight and biases with low loss, on average, across all examples. Supervisied training helps with this.

Squared loss function for linear regression:
(y - y')^2

Mean Square Error:
Sigma(y - y')^2/n

y is the ith observed value.
y' is the corresponding predicted value.
n = the number of observations.


==REDUCING LOSS==
With iterative learning, a model's updated with training to reduce loss by recalculating with each guess and getting 'warmer' to the best possible model. It takes one or more features and returns a prediction as output.
Formula is: y'=b + w_1 + x_1
Once it gets an output for a feature (w_1), it then runs a loss function for the model (see: squared loss function above).
Then, the model examines the loss value function, generates new values for b and w_1, then runs it again, over and over, until loss stops changing or changes extremely slowly.

One popular way of recalculating these paramenters is to apply the gradient descend mechanism. IN SUMMARY, it calculates the gradient of the loss curve towards the minimum. This is how it recalculates and picks new parameters to test. in ML algos, programmers tweak hyperparameters that tune the learning rate of the algo. Not too small, not too large.

BATCH: total number of examples used to calculate gradient in single iteration
STOCHASTIC GRADIENT DESCENT: batch size is 1 per iteration
MINI-BATCH STOCHASTIC GRADIENT DESCENT: typically between 10-1000 random examples


==FIRST STEPS WITH TENSORFLOW==
TensorFlow APIs have high-level APIs built on low-level APIs - the low-levels are used to make new ML algos

=> Simple Linear Regression with Synthetic Data
RULES OF THUMB FOR TUNING HYPERPARAMETERS:
- Training loss decrease should be steady. Sharp drop at first, then slowly, until curve reaches or approaches zero.
- No converge? Train for more epochs.
- Loss decrease too slowly? Try upping the learning rate. (Not too high, that might prevent convergence.)
- Loss varying wildly? Decrease learning rate.
- A good combo: lower learning rate, increase epochs or batch size.
- Try large batch sizes, then tune it down.

=> Linear Regression with a Real Dataset
* Scaling features and labels is a good idea depending on size to keep things managable, keeping loss and learning in a better range. Example:
training_df["median_house_value"] /= 1000.0

The idea is to determine what features correlate best with the label, and this is done with the dataset and the hyperparameters. Based on this you can make predictions using certain features - some are better based on the data. RULE OF THUMB: make predictions of examples not used in training.

You can also make synthetic features in pandas to use for your training and predictions. Example:
training_df["rooms_per_person"] = training_df["total_rooms"] / training_df["population"]

Another way to narrow down features to use is to use a correlation matrix: each attribute's raw values are compared to each other, and more corerelation = a higher number, from 1.0 to -1.0. Very positive correlations and very negative correlations are good predictive features.


==GENERALIZATION==
refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.

When a model DOESN'T do this (IE, low loss during training, but does a bad job predicting new data) it's called an overfitted model. A rule of thumb is this: the less complex a machine learning model, the more likely it'll make good empirical results.